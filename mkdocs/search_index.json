{
    "docs": [
        {
            "location": "/", 
            "text": "MOOSE.jl\n\n\nMultiphysics Object Oriented Simulation Environment In Julia\n\n\nThis is essentially a \"mini\" version of the true \nMOOSE multiphysics framework\n reimagined in Julia.\n\n\nMOOSE is intended to simplify the creation of of multiphysics applications.  It achieves this by breaking multiphysics problems down into small, pluggable, objects.  These small objects range from physics to boundary conditions, materials, initial conditions, etc.  By choosing a set of these small objects you can perfectly describe the simulation you want to compute.\n\n\nOne key facet of MOOSE is that although it allows for massively parallel simulations, it should never expose parallelism to the users.  Application developers should be able to focus on the equations they would like to solve while MOOSE takes care of the details of how to solve them.\n\n\nTo get started you'll want to go through the \nInstallation\n instructions.\n\n\nCode Systems\n\n\nThe idea behind MOOSE (both the real one and MOOSE.jl) is to simplify solving nonlinear, multiphysics, finite-element problems.  To do this, the problem is broken down into into small \"objects\" that are then pieced together to form the final simulation.\n\n\nTo build your simulation you inherit from an abstract type and override a few functions for your new type to specialize computations for it.\n\n\nThe main types of objects are:\n\n\n\n\nMesh\n: Holds the geometry\n\n\nSystem\n: Holds all of the pieces of the problem to be solved\n\n\nVariable\n: The variables (solution fields) you want to solve for\n\n\nKernel\n: Pieces (operators) of the PDEs you want to solve\n\n\nBoundaryCondition\n: Conditions placed on your PDEs on the boundary\n\n\nSolver\n: Linear and nonlinear solvers that solve a \nSystem\n\n\nOutput\n: Create output (normally files)\n\n\n\n\nEach of these systems can be found under their respective directories in the \n/src\n directory of the package.  In addition, some documentation can be found under the \nSystems\n menu item above.", 
            "title": "Home"
        }, 
        {
            "location": "/#moosejl", 
            "text": "Multiphysics Object Oriented Simulation Environment In Julia  This is essentially a \"mini\" version of the true  MOOSE multiphysics framework  reimagined in Julia.  MOOSE is intended to simplify the creation of of multiphysics applications.  It achieves this by breaking multiphysics problems down into small, pluggable, objects.  These small objects range from physics to boundary conditions, materials, initial conditions, etc.  By choosing a set of these small objects you can perfectly describe the simulation you want to compute.  One key facet of MOOSE is that although it allows for massively parallel simulations, it should never expose parallelism to the users.  Application developers should be able to focus on the equations they would like to solve while MOOSE takes care of the details of how to solve them.  To get started you'll want to go through the  Installation  instructions.", 
            "title": "MOOSE.jl"
        }, 
        {
            "location": "/#code-systems", 
            "text": "The idea behind MOOSE (both the real one and MOOSE.jl) is to simplify solving nonlinear, multiphysics, finite-element problems.  To do this, the problem is broken down into into small \"objects\" that are then pieced together to form the final simulation.  To build your simulation you inherit from an abstract type and override a few functions for your new type to specialize computations for it.  The main types of objects are:   Mesh : Holds the geometry  System : Holds all of the pieces of the problem to be solved  Variable : The variables (solution fields) you want to solve for  Kernel : Pieces (operators) of the PDEs you want to solve  BoundaryCondition : Conditions placed on your PDEs on the boundary  Solver : Linear and nonlinear solvers that solve a  System  Output : Create output (normally files)   Each of these systems can be found under their respective directories in the  /src  directory of the package.  In addition, some documentation can be found under the  Systems  menu item above.", 
            "title": "Code Systems"
        }, 
        {
            "location": "/installation/", 
            "text": "Installation\n\n\nMOOSE.jl currently requires a few packages that need to be manually installed.\n\n\nParallelism\n\n\nIf you are \nnot\n planning on running in parallel you can \nskip\n this section and go right to installing Julia Packages...\n\n\nTo run in parallel you need a working installation of MPI and PETSc.  MOOSE.jl has been tested with MPICH and MVAPICH, but any working MPI installation should work fine.\n\n\nPETSc\n\n\nIn addtion to MPI, you will also need a working installation of \nPETSc\n.  There are many ways to get a working installation of PETSc.  You may be able to install it through your package manager, or you could use the MOOSE redistributable package from \nStep 1\n of the \nMOOSE Getting Started guide\n.\n\n\nYour other option is to \ndownload the source\n and compile it yourself.  MOOSE.jl has been tested with PETSc version 3.6.1 but will probably work with any recent version.\n\n\nYou will need to choose a place to install PETSc and export this in your environment as \nPETSC_DIR\n.  MOOSE.jl uses \nPETSC_DIR\n to know not only where to find PETSc but also whether or not to attempt to use PETSc at all.\n\n\nTo configure PETSc we recommend something like the following:\n\n\n./configure \\\n--prefix=$PETSC_DIR \\\n--download-hypre=1 \\\n--with-ssl=0 \\\n--with-debugging=no \\\n--with-pic=1 \\\n--with-shared-libraries=1 \\\n--with-cc=mpicc \\\n--with-cxx=mpicxx \\\n--with-fc=mpif90 \\\n--download-fblaslapack=1 \\\n--download-metis=1 \\\n--download-parmetis=1 \\\n--download-superlu_dist=1 \\\n--download-mumps=1 \\\n--download-scalapack=1 \\\nCC=mpicc CXX=mpicxx FC=mpif90 F77=mpif77 F90=mpif90 \\\nCFLAGS='-fPIC -fopenmp' \\\nCXXFLAGS='-fPIC -fopenmp' \\\nFFLAGS='-fPIC -fopenmp' \\\nFCFLAGS='-fPIC -fopenmp' \\\nF90FLAGS='-fPIC -fopenmp' \\\nF77FLAGS='-fPIC -fopenmp' \\\nPETSC_DIR=`pwd`\n\n\n\n\nIn particular \n--download-metis\n is critical for MOOSE.jl.  MOOSE.jl uses METIS for mesh partitioning in parallel and will not work in parallel without it.  MOOSE.jl currently expects METIS to exist as part of your PETSC installation.\n\n\nOther than that, follow the PETSc installation instructions or you can follow the \nrecommended PETSc instructions\n for MOOSE.\n\n\nJulia Packages\n\n\nTo get all of the dependencies and MOOSE.jl... within a Julia 0.5 session do:\n\n\nPkg.clone(\nhttps://github.com/KristofferC/ContMechTensors.jl.git\n)\nPkg.clone(\nhttps://github.com/friedmud/JuAFEM.jl.git\n)\nPkg.clone(\nhttps://github.com/friedmud/DummyMPI.jl.git\n)\n\n\n\n\nIf you went through the steps to set up MPI and PETSc then you should be able to use the \nMiniPETSc\n package:\n\n\nPkg.clone(\nhttps://github.com/friedmud/MiniPETSc.jl.git\n)\n\n\n\n\nThen, installing MOOSE.jl can be done via:\n\n\nPkg.clone(\nhttps://github.com/friedmud/MOOSE.jl.git\n)\n\n\n\n\nIf you are new to Julia, you should know that these commands typically install the packages within the \n~/.julia/v0.5\n directory in your home directory.  If you navigate there you will see the \nMOOSE\n directory.  Inside of that directory are example problems and tests that can be instructive on how to use MOOSE.jl.\n\n\nBelow is a schematic showing most of the Julia packages that MOOSE.jl ultimately depends on:", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "MOOSE.jl currently requires a few packages that need to be manually installed.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#parallelism", 
            "text": "If you are  not  planning on running in parallel you can  skip  this section and go right to installing Julia Packages...  To run in parallel you need a working installation of MPI and PETSc.  MOOSE.jl has been tested with MPICH and MVAPICH, but any working MPI installation should work fine.", 
            "title": "Parallelism"
        }, 
        {
            "location": "/installation/#petsc", 
            "text": "In addtion to MPI, you will also need a working installation of  PETSc .  There are many ways to get a working installation of PETSc.  You may be able to install it through your package manager, or you could use the MOOSE redistributable package from  Step 1  of the  MOOSE Getting Started guide .  Your other option is to  download the source  and compile it yourself.  MOOSE.jl has been tested with PETSc version 3.6.1 but will probably work with any recent version.  You will need to choose a place to install PETSc and export this in your environment as  PETSC_DIR .  MOOSE.jl uses  PETSC_DIR  to know not only where to find PETSc but also whether or not to attempt to use PETSc at all.  To configure PETSc we recommend something like the following:  ./configure \\\n--prefix=$PETSC_DIR \\\n--download-hypre=1 \\\n--with-ssl=0 \\\n--with-debugging=no \\\n--with-pic=1 \\\n--with-shared-libraries=1 \\\n--with-cc=mpicc \\\n--with-cxx=mpicxx \\\n--with-fc=mpif90 \\\n--download-fblaslapack=1 \\\n--download-metis=1 \\\n--download-parmetis=1 \\\n--download-superlu_dist=1 \\\n--download-mumps=1 \\\n--download-scalapack=1 \\\nCC=mpicc CXX=mpicxx FC=mpif90 F77=mpif77 F90=mpif90 \\\nCFLAGS='-fPIC -fopenmp' \\\nCXXFLAGS='-fPIC -fopenmp' \\\nFFLAGS='-fPIC -fopenmp' \\\nFCFLAGS='-fPIC -fopenmp' \\\nF90FLAGS='-fPIC -fopenmp' \\\nF77FLAGS='-fPIC -fopenmp' \\\nPETSC_DIR=`pwd`  In particular  --download-metis  is critical for MOOSE.jl.  MOOSE.jl uses METIS for mesh partitioning in parallel and will not work in parallel without it.  MOOSE.jl currently expects METIS to exist as part of your PETSC installation.  Other than that, follow the PETSc installation instructions or you can follow the  recommended PETSc instructions  for MOOSE.", 
            "title": "PETSc"
        }, 
        {
            "location": "/installation/#julia-packages", 
            "text": "To get all of the dependencies and MOOSE.jl... within a Julia 0.5 session do:  Pkg.clone( https://github.com/KristofferC/ContMechTensors.jl.git )\nPkg.clone( https://github.com/friedmud/JuAFEM.jl.git )\nPkg.clone( https://github.com/friedmud/DummyMPI.jl.git )  If you went through the steps to set up MPI and PETSc then you should be able to use the  MiniPETSc  package:  Pkg.clone( https://github.com/friedmud/MiniPETSc.jl.git )  Then, installing MOOSE.jl can be done via:  Pkg.clone( https://github.com/friedmud/MOOSE.jl.git )  If you are new to Julia, you should know that these commands typically install the packages within the  ~/.julia/v0.5  directory in your home directory.  If you navigate there you will see the  MOOSE  directory.  Inside of that directory are example problems and tests that can be instructive on how to use MOOSE.jl.  Below is a schematic showing most of the Julia packages that MOOSE.jl ultimately depends on:", 
            "title": "Julia Packages"
        }, 
        {
            "location": "/examples/laplacian/", 
            "text": "2D Laplacian\n\n\nAs a simple example, let's solve \\(-\\nabla \\cdot \\nabla u\\) on a square domain, with \\(u=0\\) on the left and \\(u=1\\) on the right.  Just a simple Laplacian problem.\n\n\n(TLDR: If you just want to skip ahead and run it... check out the \nproblems/simple_diffusion\n directory for a pre-baked script that solves this problem)\n\n\nTo solve this problem we're going to need to:\n\n\n\n\nStart \nusing MOOSE\n\n\nCreate the Geometry\n\n\nCreate a \nSystem\n\n\nAdd \nVariable\ns to the \nSystem\n\n\nAdd \nKernel\ns representing our PDE operators acting on our \nVariable\ns to the \nSystem\n\n\nAdd some \nBoundaryCondition\n objects for the left and right boundary\n\n\nCreate a \nSolver\n\n\nWrite out the output using \nVTKOutput\n\n\nVisualize using Paraview: http://www.paraview.org\n\n\n\n\nLet's get started.  Fire up a Julia terminal and follow along:\n\n\nStart Using MOOSE\n\n\nFirst, we're going to need to use MOOSE:\n\n\nusing MOOSE\n\n\n\n\nThe Geometry\n\n\nCurrently, MOOSE.jl only has a simple built-in mesh generator.  Luckily for us, it builds rectangular, 2D domains!\n\n\nThe command is: \nbuildSquare(x_min, x_max, y_min, y_max, n_elems_x, n_elems_y)\n\n\nLet's build a 10x10 grid on the (0,1)x(0,1) unit square:\n\n\nmesh = buildSquare(0, 1, 0, 1, 10, 10);\n\n\n\n\nCreate the \nSystem\n\n\nThe \nSystem\n holds all of the information about the equations to be solved and geometry.  It must be initialized with a \nMesh\n\n\nThe only real choice when creating a \nSystem\n is what type to use for the underlying floating point type.  The parametric type you use is either \nFloat64\n or \nDual{N, Float64}\n.  The choice is dependent on whether you want your Jacobian matrices built from manually coded functions (for \nFloat64\n) or whether you want to use \nautomatic differentiation (\nDual\n)\n.\n\n\nFor now, let's use manually coded Jacobian functions:\n\n\ndiffusion_system = System{Float64}(mesh);\n\n\n\n\nAdd \nVariable\ns\n\n\nA \nVariable\n represents a field over the domain you want to solve for.  It's the \"u\" in the \n-grad^2 u\n expression.  Just like that \"u\" all \nVariable\ns have a name (\"u\", \"temperature\", \"displacement\", etc.).  To build our equations we're going to apply operators (called \nKernel\ns) to the \nVariable\ns.  But first, we need to add the \nVariable\ns to the \nSystem\n.\n\n\nIn our case, we just have one variable named \"u\":\n\n\nu = addVariable!(diffusion_system, \nu\n);\n\n\n\n\nThat line of code adds a variable named \nu\n to the \nSystem\n and returns a handle to it.  We can then use that handle to apply \nKernel\ns and \nBoundaryCondition\ns to that \nVariable\n.  Let's do that now...\n\n\nAdd \nKernel\ns\n\n\nEach term (operator) in your PDE will be represented by one or more \nKernels\n.  The \\(-\\nabla \\cdot \\nabla\\) operator (in weak form) is what is referred to as the \nDiffusion\n \nKernel\n in MOOSE.  It's already built-in (see \nsrc/kernels/Diffusion.jl\n so the only thing we need to do is apply that operator to our new variable (\nu\n):\n\n\naddKernel!(diffusion_system, Diffusion(u));\n\n\n\n\nThe \nDiffusion(u)\n part of that statement creates a \nDiffusion\n \nKernel\n that is acting on our \nu\n variable.  It's then added to our \ndiffusion_system\n.\n\n\nIf we had more terms in our PDE we would continue to call \naddKernel!()\n, creating each term and applying it to the appropriate \nVariable\n.\n\n\nThis brings up a good point about MOOSE.  All of the objects are \"reusable\".  The \nDiffusion\n \nKernel\n represents the \"idea\" of the \\(-\\nabla\\cdot\\nabla\\) operator.  That \nDiffusion\n \nKernel\n can be applied to as many variables as you want.  So, if you are solving 16 equations and a \\(-\\nabla\\cdot\\nabla\\) shows up in each one... then you can apply the same \nDiffusion\n operator to each variable.  This gives us a large amount of code reuse and flexibility.\n\n\nBoundaryCondition\ns\n\n\nNow that we've taken care of our PDE operators, we need to handle the boundary conditions \n(BCs)\n.  We said we wanted \\(u=0\\) on the left of the domain and \\(u=1\\) on the right.  A \\(u=something\\) BC (aka \"essential\" BC, aka \"BC of the first kind\", etc.) is a type of \nBoundaryCondition\n we refer to as a \nDirichletBC\n.  Just as with \nKernel\ns we can reuse the same object, applying it on different boundaries with different values.\n\n\nSpeaking of \"boundaries\"... our built-in mesh generator automatically added some \"sidesets\" and \"nodesets\" in our \nMesh\n.  These are sets of boundary geometry that allow us to specify where \nBoundaryCondition\ns are applied.  \nDirichletBC\n objects operate on \"nodesets\".\n\n\nFor the built-in mesh generator the sidesets/nodesets are as follows:\n\n\n\n\n1: Bottom\n\n\n2: Right\n\n\n3: Top\n\n\n4: Left\n\n\n\n\nBy applying \nBoundaryCondition\n objects to those \"boundary IDs\" we can select the part of the domain the BC will be applied on.\n\n\nIn our case we need two \nDirichletBC\ns... one on the left and one on the right with the proper values:\n\n\naddBC!(diffusion_system, DirichletBC(u, [4], 0.0));\naddBC!(diffusion_system, DirichletBC(u, [2], 1.0));\n\n\n\n\nAs you can see, a \nBoundaryCondition\n (like a \nKernel\n) first takes the \nVariable\n it's going to operate on.  Next, comes an array of the boundary IDs it will be applied on and finally, in the case of \nDirichletBC\n, the last argument specifies the value to enforce.\n\n\nInitialization\n\n\nOur set of equations is now complete.  We've created geometry, added operators and set boundary conditions.  Before we can continue we need to \ninitialize!()\n the \nSystem\n:\n\n\ninitialize!(diffusion_system);\n\n\n\n\nThis step can do a lot of things... but the main thing it does is distribute all of the Degrees of Freedom (DoFs) corresponding to our \nVariable\ns across our \nMesh\n.  The DoFs are the actually finite-element coefficients we'll be solving for in the next step...\n\n\nSolve\n\n\nNow that the problem is setup we need to actually solve the system of equations.  To do that we're going to create a \nSolver\n and \nsolve!()\n it.\n\n\nFor now, we're just going to use a simple, direct, built-in linear solver: \nJuliaDenseImplicitSolver\n:\n\n\nsolver = JuliaDenseImplicitSolver(diffusion_system);\nsolve!(solver);\n\n\n\n\nAs you can see, we needed to tell it which \nSystem\n to solve... and then call \nsolve!()\n on the new \nSolver\n to actually do the solve.\n\n\nOutput\n\n\nOnce this is complete the solution will be held within the \nsolver\n object.  To output it to a file so we can view it with \nParaview\n we can do:\n\n\nout = VTKOutput();\noutput(out, solver, \nsimple_diffusion_out\n);\n\n\n\n\n\"VTK\" is a visualization file format that many third-party visualization tools (such as Paraview) can read.\n\n\nFinal File\n\n\nAfter all of these steps your file should look like:\n\n\nusing MOOSE\n\n# Create the Mesh\nmesh = buildSquare(0, 1, 0, 1, 10, 10)\n\n# Create the System to hold the equations\ndiffusion_system = System{Float64}(mesh)\n\n# Add a variable to solve for\nu = addVariable!(diffusion_system, \nu\n)\n\n# Apply the Laplacian operator to the variable\naddKernel!(diffusion_system, Diffusion(u))\n\n# u = 0 on the Left\naddBC!(diffusion_system, DirichletBC(u, [4], 0.0))\n\n# u = 1 on the Right\naddBC!(diffusion_system, DirichletBC(u, [2], 1.0))\n\n# Initialize the system of equations\ninitialize!(diffusion_system)\n\n# Create a solver and solve\nsolver = JuliaDenseImplicitSolver(diffusion_system)\nsolve!(solver)\n\n# Output\nout = VTKOutput()\noutput(out, solver, \nsimple_diffusion_out\n)\n\n\n\n\nRunning It\n\n\nSave the file as \ndiffusion.jl\n and run it like so:\n\n\njulia diffusion.jl\n\n\n\n\nThat last step should have produced a \nsimple_diffusion_out.vtu\n file in your directory.  You can then use \nParaview\n to open the file and view the result:", 
            "title": "1. Laplacian"
        }, 
        {
            "location": "/examples/laplacian/#2d-laplacian", 
            "text": "As a simple example, let's solve \\(-\\nabla \\cdot \\nabla u\\) on a square domain, with \\(u=0\\) on the left and \\(u=1\\) on the right.  Just a simple Laplacian problem.  (TLDR: If you just want to skip ahead and run it... check out the  problems/simple_diffusion  directory for a pre-baked script that solves this problem)  To solve this problem we're going to need to:   Start  using MOOSE  Create the Geometry  Create a  System  Add  Variable s to the  System  Add  Kernel s representing our PDE operators acting on our  Variable s to the  System  Add some  BoundaryCondition  objects for the left and right boundary  Create a  Solver  Write out the output using  VTKOutput  Visualize using Paraview: http://www.paraview.org   Let's get started.  Fire up a Julia terminal and follow along:", 
            "title": "2D Laplacian"
        }, 
        {
            "location": "/examples/laplacian/#start-using-moose", 
            "text": "First, we're going to need to use MOOSE:  using MOOSE", 
            "title": "Start Using MOOSE"
        }, 
        {
            "location": "/examples/laplacian/#the-geometry", 
            "text": "Currently, MOOSE.jl only has a simple built-in mesh generator.  Luckily for us, it builds rectangular, 2D domains!  The command is:  buildSquare(x_min, x_max, y_min, y_max, n_elems_x, n_elems_y)  Let's build a 10x10 grid on the (0,1)x(0,1) unit square:  mesh = buildSquare(0, 1, 0, 1, 10, 10);", 
            "title": "The Geometry"
        }, 
        {
            "location": "/examples/laplacian/#create-the-system", 
            "text": "The  System  holds all of the information about the equations to be solved and geometry.  It must be initialized with a  Mesh  The only real choice when creating a  System  is what type to use for the underlying floating point type.  The parametric type you use is either  Float64  or  Dual{N, Float64} .  The choice is dependent on whether you want your Jacobian matrices built from manually coded functions (for  Float64 ) or whether you want to use  automatic differentiation ( Dual ) .  For now, let's use manually coded Jacobian functions:  diffusion_system = System{Float64}(mesh);", 
            "title": "Create the System"
        }, 
        {
            "location": "/examples/laplacian/#add-variables", 
            "text": "A  Variable  represents a field over the domain you want to solve for.  It's the \"u\" in the  -grad^2 u  expression.  Just like that \"u\" all  Variable s have a name (\"u\", \"temperature\", \"displacement\", etc.).  To build our equations we're going to apply operators (called  Kernel s) to the  Variable s.  But first, we need to add the  Variable s to the  System .  In our case, we just have one variable named \"u\":  u = addVariable!(diffusion_system,  u );  That line of code adds a variable named  u  to the  System  and returns a handle to it.  We can then use that handle to apply  Kernel s and  BoundaryCondition s to that  Variable .  Let's do that now...", 
            "title": "Add Variables"
        }, 
        {
            "location": "/examples/laplacian/#add-kernels", 
            "text": "Each term (operator) in your PDE will be represented by one or more  Kernels .  The \\(-\\nabla \\cdot \\nabla\\) operator (in weak form) is what is referred to as the  Diffusion   Kernel  in MOOSE.  It's already built-in (see  src/kernels/Diffusion.jl  so the only thing we need to do is apply that operator to our new variable ( u ):  addKernel!(diffusion_system, Diffusion(u));  The  Diffusion(u)  part of that statement creates a  Diffusion   Kernel  that is acting on our  u  variable.  It's then added to our  diffusion_system .  If we had more terms in our PDE we would continue to call  addKernel!() , creating each term and applying it to the appropriate  Variable .  This brings up a good point about MOOSE.  All of the objects are \"reusable\".  The  Diffusion   Kernel  represents the \"idea\" of the \\(-\\nabla\\cdot\\nabla\\) operator.  That  Diffusion   Kernel  can be applied to as many variables as you want.  So, if you are solving 16 equations and a \\(-\\nabla\\cdot\\nabla\\) shows up in each one... then you can apply the same  Diffusion  operator to each variable.  This gives us a large amount of code reuse and flexibility.", 
            "title": "Add Kernels"
        }, 
        {
            "location": "/examples/laplacian/#boundaryconditions", 
            "text": "Now that we've taken care of our PDE operators, we need to handle the boundary conditions  (BCs) .  We said we wanted \\(u=0\\) on the left of the domain and \\(u=1\\) on the right.  A \\(u=something\\) BC (aka \"essential\" BC, aka \"BC of the first kind\", etc.) is a type of  BoundaryCondition  we refer to as a  DirichletBC .  Just as with  Kernel s we can reuse the same object, applying it on different boundaries with different values.  Speaking of \"boundaries\"... our built-in mesh generator automatically added some \"sidesets\" and \"nodesets\" in our  Mesh .  These are sets of boundary geometry that allow us to specify where  BoundaryCondition s are applied.   DirichletBC  objects operate on \"nodesets\".  For the built-in mesh generator the sidesets/nodesets are as follows:   1: Bottom  2: Right  3: Top  4: Left   By applying  BoundaryCondition  objects to those \"boundary IDs\" we can select the part of the domain the BC will be applied on.  In our case we need two  DirichletBC s... one on the left and one on the right with the proper values:  addBC!(diffusion_system, DirichletBC(u, [4], 0.0));\naddBC!(diffusion_system, DirichletBC(u, [2], 1.0));  As you can see, a  BoundaryCondition  (like a  Kernel ) first takes the  Variable  it's going to operate on.  Next, comes an array of the boundary IDs it will be applied on and finally, in the case of  DirichletBC , the last argument specifies the value to enforce.", 
            "title": "BoundaryConditions"
        }, 
        {
            "location": "/examples/laplacian/#initialization", 
            "text": "Our set of equations is now complete.  We've created geometry, added operators and set boundary conditions.  Before we can continue we need to  initialize!()  the  System :  initialize!(diffusion_system);  This step can do a lot of things... but the main thing it does is distribute all of the Degrees of Freedom (DoFs) corresponding to our  Variable s across our  Mesh .  The DoFs are the actually finite-element coefficients we'll be solving for in the next step...", 
            "title": "Initialization"
        }, 
        {
            "location": "/examples/laplacian/#solve", 
            "text": "Now that the problem is setup we need to actually solve the system of equations.  To do that we're going to create a  Solver  and  solve!()  it.  For now, we're just going to use a simple, direct, built-in linear solver:  JuliaDenseImplicitSolver :  solver = JuliaDenseImplicitSolver(diffusion_system);\nsolve!(solver);  As you can see, we needed to tell it which  System  to solve... and then call  solve!()  on the new  Solver  to actually do the solve.", 
            "title": "Solve"
        }, 
        {
            "location": "/examples/laplacian/#output", 
            "text": "Once this is complete the solution will be held within the  solver  object.  To output it to a file so we can view it with  Paraview  we can do:  out = VTKOutput();\noutput(out, solver,  simple_diffusion_out );  \"VTK\" is a visualization file format that many third-party visualization tools (such as Paraview) can read.", 
            "title": "Output"
        }, 
        {
            "location": "/examples/laplacian/#final-file", 
            "text": "After all of these steps your file should look like:  using MOOSE\n\n# Create the Mesh\nmesh = buildSquare(0, 1, 0, 1, 10, 10)\n\n# Create the System to hold the equations\ndiffusion_system = System{Float64}(mesh)\n\n# Add a variable to solve for\nu = addVariable!(diffusion_system,  u )\n\n# Apply the Laplacian operator to the variable\naddKernel!(diffusion_system, Diffusion(u))\n\n# u = 0 on the Left\naddBC!(diffusion_system, DirichletBC(u, [4], 0.0))\n\n# u = 1 on the Right\naddBC!(diffusion_system, DirichletBC(u, [2], 1.0))\n\n# Initialize the system of equations\ninitialize!(diffusion_system)\n\n# Create a solver and solve\nsolver = JuliaDenseImplicitSolver(diffusion_system)\nsolve!(solver)\n\n# Output\nout = VTKOutput()\noutput(out, solver,  simple_diffusion_out )", 
            "title": "Final File"
        }, 
        {
            "location": "/examples/laplacian/#running-it", 
            "text": "Save the file as  diffusion.jl  and run it like so:  julia diffusion.jl  That last step should have produced a  simple_diffusion_out.vtu  file in your directory.  You can then use  Paraview  to open the file and view the result:", 
            "title": "Running It"
        }, 
        {
            "location": "/examples/auto_diff/", 
            "text": "Automatic Diferentiation\n\n\nThis example builds on \nthe Laplacian example\n.\n\n\nLooking at \nDiffusion.jl\n\n\nIn that example the \nDiffusion\n Kernel was used to apply a \n-div grad\n to \nu\n.  The code for that Kernel can be \nviewed on GitHub\n.\n\n\nA \nKernel\n is a piece of physics.  Typically one term in a PDE.  It represents the mathematical operator and is generic and can be applied to many different variables.\n\n\nThe implementation of a \nKernel\n consists of a \ntype\n that is a sub-type of \nKernel\n.  In this case:\n\n\ntype Diffusion \n: Kernel\n    u::Variable\nend\n\n\n\n\nIn addition a number of other functions can be specified which work on that type.  In particular:\n\n\n\n\ncomputeQpResdiual()\n: computes the finite element residual at one quadrature point.\n\n\ncomputeQpJacobian()\n: computes the derivative of the residual with respec to the passed in variable.\n\n\n\n\nFor the \nDiffusion\n Kernel they look like:\n\n\n@inline function computeQpResidual(kernel::Diffusion, qp::Integer, i::Integer)\n    u = kernel.u\n\n    return u.grad[qp] \u22c5 u.grad_phi[qp][i]\nend\n\n@inline function computeQpJacobian(kernel::Diffusion, v::Variable, qp::Integer, i::Integer, j::Integer)::Float64\n    u = kernel.u\n\n    if u.id == v.id\n        return v.grad_phi[qp][j] \u22c5 u.grad_phi[qp][i]\n    end\n\n    return 0\nend\n\n\n\n\nWhile, in this instance, the \ncomputeQpJacobian()\n function is straightforward to write, it can become incredibly difficult to derive for complex operators.  In addition, derivation of Jacobian terms is one of the most error-prone pieces of any finite-elemnt code... even moreso for a multiphysics code (because derivatives must be computed with respect to every variable in the system).  For this reason it would be extremely advantageous if the derivatives could be computed automatically.\n\n\nAutomatic Differentiation (AD)\n\n\nIt turns out that \ncomputeQpJacobian()\n can be automatically computed by utilizing \nForwardDiff.jl\n's \nDual\n type.  When working in this mode the \ncomputeQpJacobian()\n function will NOT be called at all... and therefore it is not necessary to even specify one.\n\n\nThere is only one small change in your script that is necessary for using AD: the parametric datatype for the \nSystem\n object must be changed.  In the \nLaplacian Example\n the \nSystem\n was created like so:\n\n\ndiffusion_system = System{Float64}(mesh)\n\n\n\n\nThe \nFloat64\n in that declaration is specifying the intrinsic datatype the residual is computed with.  To use AD, all that is necessary is simply:\n\n\ndiffusion_system = System{Dual{4,Float64}}(mesh)\n\n\n\n\nThat will cause MOOSE.jl to utilize \nForwardDiff.jl\n for automatic computation of Jacobian entries.  Analytic derivatives are a thing of the past!\n\n\nHowever: what is that \n4\n in the declaration?  The \n4\n is the maximum number of degrees of freedom (DoFs) on any one element.  It specifies the storage size inside of \nDual\n for derivative information.\n\n\nIn this particular case we have only one variable... and we are using Quad4 elements.  Therefore, we will have \n4\n degrees of freedom on each element... hence the choice of \n4\n here.  If, however, we had two variables then this would need to be \n8\n, three would be \n12\n, etc.\n\n\nIn the future we hope to improve the automatic detection of this number... but for now it needs to be set manually.\n\n\nPutting It All Together\n\n\nIn summary, to use AD all that is needed is to:\n\n\n\n\nCreate Kernels that provide \ncomputeQpResidual()\n  (\ncomputeQpJacobian()\n is unnecessary)\n\n\nChange the parametric type of the \nSystem\n object in your script to \nDual\n\n\nChoose the correct amount of storage for partial derivatives insidue of a \nDual\n\n\n\n\nHere is the full Laplacian script that uses automatic differentiation:\n\n\nusing MOOSE\n\n# Create the Mesh\nmesh = buildSquare(0, 1, 0, 1, 10, 10)\n\n# Create the System to hold the equations\ndiffusion_system = System{Dual{4,Float64}}(mesh)\n\n# Add a variable to solve for\nu = addVariable!(diffusion_system, \nu\n)\n\n# Apply the Laplacian operator to the variable\naddKernel!(diffusion_system, Diffusion(u))\n\n# u = 0 on the Left\naddBC!(diffusion_system, DirichletBC(u, [4], 0.0))\n\n# u = 1 on the Right\naddBC!(diffusion_system, DirichletBC(u, [2], 1.0))\n\n# Initialize the system of equations\ninitialize!(diffusion_system)\n\n# Create a solver and solve\nsolver = JuliaDenseImplicitSolver(diffusion_system)\nsolve!(solver)\n\n# Output\nout = VTKOutput()\noutput(out, solver, \nsimple_diffusion_out\n)", 
            "title": "2. Auto-Diff"
        }, 
        {
            "location": "/examples/auto_diff/#automatic-diferentiation", 
            "text": "This example builds on  the Laplacian example .", 
            "title": "Automatic Diferentiation"
        }, 
        {
            "location": "/examples/auto_diff/#looking-at-diffusionjl", 
            "text": "In that example the  Diffusion  Kernel was used to apply a  -div grad  to  u .  The code for that Kernel can be  viewed on GitHub .  A  Kernel  is a piece of physics.  Typically one term in a PDE.  It represents the mathematical operator and is generic and can be applied to many different variables.  The implementation of a  Kernel  consists of a  type  that is a sub-type of  Kernel .  In this case:  type Diffusion  : Kernel\n    u::Variable\nend  In addition a number of other functions can be specified which work on that type.  In particular:   computeQpResdiual() : computes the finite element residual at one quadrature point.  computeQpJacobian() : computes the derivative of the residual with respec to the passed in variable.   For the  Diffusion  Kernel they look like:  @inline function computeQpResidual(kernel::Diffusion, qp::Integer, i::Integer)\n    u = kernel.u\n\n    return u.grad[qp] \u22c5 u.grad_phi[qp][i]\nend\n\n@inline function computeQpJacobian(kernel::Diffusion, v::Variable, qp::Integer, i::Integer, j::Integer)::Float64\n    u = kernel.u\n\n    if u.id == v.id\n        return v.grad_phi[qp][j] \u22c5 u.grad_phi[qp][i]\n    end\n\n    return 0\nend  While, in this instance, the  computeQpJacobian()  function is straightforward to write, it can become incredibly difficult to derive for complex operators.  In addition, derivation of Jacobian terms is one of the most error-prone pieces of any finite-elemnt code... even moreso for a multiphysics code (because derivatives must be computed with respect to every variable in the system).  For this reason it would be extremely advantageous if the derivatives could be computed automatically.", 
            "title": "Looking at Diffusion.jl"
        }, 
        {
            "location": "/examples/auto_diff/#automatic-differentiation-ad", 
            "text": "It turns out that  computeQpJacobian()  can be automatically computed by utilizing  ForwardDiff.jl 's  Dual  type.  When working in this mode the  computeQpJacobian()  function will NOT be called at all... and therefore it is not necessary to even specify one.  There is only one small change in your script that is necessary for using AD: the parametric datatype for the  System  object must be changed.  In the  Laplacian Example  the  System  was created like so:  diffusion_system = System{Float64}(mesh)  The  Float64  in that declaration is specifying the intrinsic datatype the residual is computed with.  To use AD, all that is necessary is simply:  diffusion_system = System{Dual{4,Float64}}(mesh)  That will cause MOOSE.jl to utilize  ForwardDiff.jl  for automatic computation of Jacobian entries.  Analytic derivatives are a thing of the past!  However: what is that  4  in the declaration?  The  4  is the maximum number of degrees of freedom (DoFs) on any one element.  It specifies the storage size inside of  Dual  for derivative information.  In this particular case we have only one variable... and we are using Quad4 elements.  Therefore, we will have  4  degrees of freedom on each element... hence the choice of  4  here.  If, however, we had two variables then this would need to be  8 , three would be  12 , etc.  In the future we hope to improve the automatic detection of this number... but for now it needs to be set manually.", 
            "title": "Automatic Differentiation (AD)"
        }, 
        {
            "location": "/examples/auto_diff/#putting-it-all-together", 
            "text": "In summary, to use AD all that is needed is to:   Create Kernels that provide  computeQpResidual()   ( computeQpJacobian()  is unnecessary)  Change the parametric type of the  System  object in your script to  Dual  Choose the correct amount of storage for partial derivatives insidue of a  Dual   Here is the full Laplacian script that uses automatic differentiation:  using MOOSE\n\n# Create the Mesh\nmesh = buildSquare(0, 1, 0, 1, 10, 10)\n\n# Create the System to hold the equations\ndiffusion_system = System{Dual{4,Float64}}(mesh)\n\n# Add a variable to solve for\nu = addVariable!(diffusion_system,  u )\n\n# Apply the Laplacian operator to the variable\naddKernel!(diffusion_system, Diffusion(u))\n\n# u = 0 on the Left\naddBC!(diffusion_system, DirichletBC(u, [4], 0.0))\n\n# u = 1 on the Right\naddBC!(diffusion_system, DirichletBC(u, [2], 1.0))\n\n# Initialize the system of equations\ninitialize!(diffusion_system)\n\n# Create a solver and solve\nsolver = JuliaDenseImplicitSolver(diffusion_system)\nsolve!(solver)\n\n# Output\nout = VTKOutput()\noutput(out, solver,  simple_diffusion_out )", 
            "title": "Putting It All Together"
        }, 
        {
            "location": "/examples/petsc/", 
            "text": "PETSc\n\n\nWhile Julia has quite a lot of linear algebra capability built-in, it is still missing a few things.  In particular, Julia does not contain built-in iterative linear solvers (like Krylov solvers).  Even though there are some third-party Krylov solvers (such \nKrylovMethods.jl\n ) they lack preconditioning and parallelism.\n\n\nFor this reason, it is useful to utilize the \nPETSc library\n from Argonne National Laboratory.  PETSc is a mature code-base providing parallel Krylov solvers and preconditioners that are a perfect match for FEM.  To use them, I have developed a Julia wrapper: \nMiniPETSc.jl\n which MOOSE.jl takes advantage of.\n\n\nPETSc Setup\n\n\nBefore using PETSc it's critical that you've been through the \nsetup steps on the Installation page\n.  PETSc needs to be compiled against your MPI installation, installed and the \nPETSC_DIR\n environment variable needs to be set to point to that installation.\n\n\nUsing PETSc\n\n\nTo use PETSc within your MOOSE.jl script... all that is necessary is to switch the type of solver you are using to one of the \nPetsc*\n solvers.  For instance, here is a modified form of the script from Example #2:\n\n\nusing MOOSE\n\n# Create the Mesh\nmesh = buildSquare(0, 1, 0, 1, 10, 10)\n\n# Create the System to hold the equations\ndiffusion_system = System{Dual{4,Float64}}(mesh)\n\n# Add a variable to solve for\nu = addVariable!(diffusion_system, \nu\n)\n\n# Apply the Laplacian operator to the variable\naddKernel!(diffusion_system, Diffusion(u))\n\n# u = 0 on the Left\naddBC!(diffusion_system, DirichletBC(u, [4], 0.0))\n\n# u = 1 on the Right\naddBC!(diffusion_system, DirichletBC(u, [2], 1.0))\n\n# Initialize the system of equations\ninitialize!(diffusion_system)\n\n# Create a solver and solve\nsolver = PetscImplicitSolver(diffusion_system)\nsolve!(solver)\n\n# Output\nout = VTKOutput()\noutput(out, solver, \nsimple_diffusion_out\n)\n\n\n\n\nThe \nPetscImplicitSolver\n will utilize PETSc vectors, matrices and solvers to solve the system.\n\n\nPETSc Options\n\n\nBy default PETSc will use a GMRES solver with ILU-0 preconditioning.  For small, simple problems this will work fine.  But: for anything more complicated you will want to specify options to PETSc to change the solver/preconditioner.  This can be achieved using the normal PETSc command-line syntax.\n\n\nFor instance, to use the algebraic multigrid \nHypre/Boomeramg\n package for preconditioning you would run your script like so:\n\n\njulia myscript.jl -pc_type hypre -pc_hypre_type boomeramg\n\n\n\n\nTo see a lot of the command-line options you can use try running your script using the \n-help\n option:\n\n\njulia myscript.jl -help\n\n\n\n\nHere are some of the most useful/common PETSc options:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-ksp_monitor\n\n\nView convergence information\n\n\n\n\n\n\n-snes_ksp_ew\n\n\nVariable linear solve tolerance, useful for transient solves\n\n\n\n\n\n\n-help\n\n\nShow PETSc options during the solve\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-pc-type\n\n\nilu\n\n\nDefault for serial\n\n\n\n\n\n\n\n\nbjacobi\n\n\nDefault for parallel with \n-sub_pc_type ilu\n\n\n\n\n\n\n\n\nasm\n\n\nAdditive Schwartz with \n-sub_pc_type ilu\n\n\n\n\n\n\n\n\nlu\n\n\nFull LU, serial only\n\n\n\n\n\n\n\n\ngamg\n\n\nGeneralized Geometric-Algebric MultiGrid\n\n\n\n\n\n\n\n\nhypre\n\n\nHypre, usually used with \nboomeramg\n\n\n\n\n\n\n-sub_pc_type\n\n\nilu, lu, hypre\n\n\nCan be used with \nbjacobi\n or \nasm\n\n\n\n\n\n\n-pc_hypre_type\n\n\nboomeramg\n\n\nAlgebraic Multigrid\n\n\n\n\n\n\n-ksp_gmres_restart\n\n\n# (default = 30)\n\n\nNumber of Krylov vectors to store\n\n\n\n\n\n\n\n\nFor even more, refer to the \nPETSc documentation", 
            "title": "3. PETSc"
        }, 
        {
            "location": "/examples/petsc/#petsc", 
            "text": "While Julia has quite a lot of linear algebra capability built-in, it is still missing a few things.  In particular, Julia does not contain built-in iterative linear solvers (like Krylov solvers).  Even though there are some third-party Krylov solvers (such  KrylovMethods.jl  ) they lack preconditioning and parallelism.  For this reason, it is useful to utilize the  PETSc library  from Argonne National Laboratory.  PETSc is a mature code-base providing parallel Krylov solvers and preconditioners that are a perfect match for FEM.  To use them, I have developed a Julia wrapper:  MiniPETSc.jl  which MOOSE.jl takes advantage of.", 
            "title": "PETSc"
        }, 
        {
            "location": "/examples/petsc/#petsc-setup", 
            "text": "Before using PETSc it's critical that you've been through the  setup steps on the Installation page .  PETSc needs to be compiled against your MPI installation, installed and the  PETSC_DIR  environment variable needs to be set to point to that installation.", 
            "title": "PETSc Setup"
        }, 
        {
            "location": "/examples/petsc/#using-petsc", 
            "text": "To use PETSc within your MOOSE.jl script... all that is necessary is to switch the type of solver you are using to one of the  Petsc*  solvers.  For instance, here is a modified form of the script from Example #2:  using MOOSE\n\n# Create the Mesh\nmesh = buildSquare(0, 1, 0, 1, 10, 10)\n\n# Create the System to hold the equations\ndiffusion_system = System{Dual{4,Float64}}(mesh)\n\n# Add a variable to solve for\nu = addVariable!(diffusion_system,  u )\n\n# Apply the Laplacian operator to the variable\naddKernel!(diffusion_system, Diffusion(u))\n\n# u = 0 on the Left\naddBC!(diffusion_system, DirichletBC(u, [4], 0.0))\n\n# u = 1 on the Right\naddBC!(diffusion_system, DirichletBC(u, [2], 1.0))\n\n# Initialize the system of equations\ninitialize!(diffusion_system)\n\n# Create a solver and solve\nsolver = PetscImplicitSolver(diffusion_system)\nsolve!(solver)\n\n# Output\nout = VTKOutput()\noutput(out, solver,  simple_diffusion_out )  The  PetscImplicitSolver  will utilize PETSc vectors, matrices and solvers to solve the system.", 
            "title": "Using PETSc"
        }, 
        {
            "location": "/examples/petsc/#petsc-options", 
            "text": "By default PETSc will use a GMRES solver with ILU-0 preconditioning.  For small, simple problems this will work fine.  But: for anything more complicated you will want to specify options to PETSc to change the solver/preconditioner.  This can be achieved using the normal PETSc command-line syntax.  For instance, to use the algebraic multigrid  Hypre/Boomeramg  package for preconditioning you would run your script like so:  julia myscript.jl -pc_type hypre -pc_hypre_type boomeramg  To see a lot of the command-line options you can use try running your script using the  -help  option:  julia myscript.jl -help  Here are some of the most useful/common PETSc options:     Name  Description      -ksp_monitor  View convergence information    -snes_ksp_ew  Variable linear solve tolerance, useful for transient solves    -help  Show PETSc options during the solve        Name  Value  Description      -pc-type  ilu  Default for serial     bjacobi  Default for parallel with  -sub_pc_type ilu     asm  Additive Schwartz with  -sub_pc_type ilu     lu  Full LU, serial only     gamg  Generalized Geometric-Algebric MultiGrid     hypre  Hypre, usually used with  boomeramg    -sub_pc_type  ilu, lu, hypre  Can be used with  bjacobi  or  asm    -pc_hypre_type  boomeramg  Algebraic Multigrid    -ksp_gmres_restart  # (default = 30)  Number of Krylov vectors to store     For even more, refer to the  PETSc documentation", 
            "title": "PETSc Options"
        }, 
        {
            "location": "/examples/parallel/", 
            "text": "Going Parallel\n\n\nThis example builds off of the \nPETSc Example\n.  To run in parallel you must have \nPETSc\n setup as described in the \nInstallation Guide\n.\n\n\nMPI\n\n\nMPI stands for \"Message Passing Interface\".  It is \na standard\n describing how computers can efficiently send messages to eachother when working in a cluster environment.  Although it is still evolving it is also extremely mature having been around for more than 20 years.\n\n\nMOOSE.jl uses MPI through \nMPI.jl\n: MPI bindings for Julia.  It picks up MPI.jl through \nMiniPETSc.jl\n: the Julia PETSc bindings.  Using MPI, MOOSE.jl can scale to thousands of processors on large clusters.\n\n\nMesh Partitioning\n\n\nThe most straightforward way to parallelize a FEM solve is by splitting the domain (the elements) up over the available processors.  In this way each processor receives a portion of the domain to work on and the load is balanced.  However, just choosing any random splitting of elements is non-optimal.  Communication overhead can ruin parallel scalability.  Therefore it is necessary to seek domain splittings that minimize the amount of communication.  This process is called: \"Partitioning\" the mesh.\n\n\nMOOSE.jl utilize a library called \nMETIS\n for this purpose.  METIS is very mature graph partitioning software.  Given the connectivity graph METIS will attempt to solve for an optimal partitioning that balances the load and reduces communication costs.\n\n\nMOOSE.jl retrieves METIS through Julia bindings that are currently located within MOOSE.jl (but may be moved to their own package later).  Currently, it is required that METIS be built into the PETSc library you are using (see \nInstallation\n).\n\n\nRunning in Parallel\n\n\nWith MPI, PETSc and METIS in place: the only thing you need to do to run a MOOSE.jl script in parallel is to make sure you are using a \nPetsc*\n solver (like \nPetscImplicitSolver\n) and then launch your script using \nmpiexec\n (or \nmpirun\n depending on your MPI installation):\n\n\nmpiexec -n 4 julia yourscript.jl\n\n\n\n\nThat will launch \n4\n MPI processes that will all work together to solve the problem.\n\n\nThat's it!  No other code needs to change!\n\n\nSmall Issue: \"Compiling\"\n\n\nUnfortunately: Julia does not really expect to be launched simultaneously like this... and one of the things Julia does (pre-compiling Packages) can run into trouble.  The issue is that if you have modified (or just installed) a Julia Package, the first time you attempt to run a script that uses it Julia will \"pre-compile\" that package to make it faster to launch scripts that use it in the future.\n\n\nUnfortunately, when Julia does this it tries to write files within a directory in your home directory.  If multiple Julia instances are launched simultaneously they will ALL attempt to precompile the package and all attempt to overwrite eachother's pre-compiled files.  This leads to crazy errors.\n\n\nTo combat this: always make sure to run your MOOSE.jl scripts \nwithout\n \nmpiexec\n first... to get MOOSE.jl (and everything it depends on) to pre-compile in serial.  Then you can run in parallel.\n\n\nIn fact: it's not necessary to run a full solve.  Simply create a file that has this in it (I call mine \ncompile.jl\n):\n\n\nusing MOOSE\n\n\n\n\nAnd then you can execute your real script like this:\n\n\njulia compile.jl \n mpiexec -n 4 julia myscript.jl\n\n\n\n\nThat will run the short \ncompile.jl\n script in serial first... ensuring that MOOSE.jl is compiled and then launch the real script in parallel...\n\n\nScalability\n\n\nScalability of MOOSE.jl depends quite a lot on the linear solver/preconditioner you choose to use with PETSc and the particular problem you are solving.  That said, the finite-element assembly part of MOOSE.jl does scale well.  The result below is for a coupled set of 25 PDEs being assembled on a 400x400 element mesh on up to 3,072 processors.", 
            "title": "4. Parallel"
        }, 
        {
            "location": "/examples/parallel/#going-parallel", 
            "text": "This example builds off of the  PETSc Example .  To run in parallel you must have  PETSc  setup as described in the  Installation Guide .", 
            "title": "Going Parallel"
        }, 
        {
            "location": "/examples/parallel/#mpi", 
            "text": "MPI stands for \"Message Passing Interface\".  It is  a standard  describing how computers can efficiently send messages to eachother when working in a cluster environment.  Although it is still evolving it is also extremely mature having been around for more than 20 years.  MOOSE.jl uses MPI through  MPI.jl : MPI bindings for Julia.  It picks up MPI.jl through  MiniPETSc.jl : the Julia PETSc bindings.  Using MPI, MOOSE.jl can scale to thousands of processors on large clusters.", 
            "title": "MPI"
        }, 
        {
            "location": "/examples/parallel/#mesh-partitioning", 
            "text": "The most straightforward way to parallelize a FEM solve is by splitting the domain (the elements) up over the available processors.  In this way each processor receives a portion of the domain to work on and the load is balanced.  However, just choosing any random splitting of elements is non-optimal.  Communication overhead can ruin parallel scalability.  Therefore it is necessary to seek domain splittings that minimize the amount of communication.  This process is called: \"Partitioning\" the mesh.  MOOSE.jl utilize a library called  METIS  for this purpose.  METIS is very mature graph partitioning software.  Given the connectivity graph METIS will attempt to solve for an optimal partitioning that balances the load and reduces communication costs.  MOOSE.jl retrieves METIS through Julia bindings that are currently located within MOOSE.jl (but may be moved to their own package later).  Currently, it is required that METIS be built into the PETSc library you are using (see  Installation ).", 
            "title": "Mesh Partitioning"
        }, 
        {
            "location": "/examples/parallel/#running-in-parallel", 
            "text": "With MPI, PETSc and METIS in place: the only thing you need to do to run a MOOSE.jl script in parallel is to make sure you are using a  Petsc*  solver (like  PetscImplicitSolver ) and then launch your script using  mpiexec  (or  mpirun  depending on your MPI installation):  mpiexec -n 4 julia yourscript.jl  That will launch  4  MPI processes that will all work together to solve the problem.  That's it!  No other code needs to change!", 
            "title": "Running in Parallel"
        }, 
        {
            "location": "/examples/parallel/#small-issue-compiling", 
            "text": "Unfortunately: Julia does not really expect to be launched simultaneously like this... and one of the things Julia does (pre-compiling Packages) can run into trouble.  The issue is that if you have modified (or just installed) a Julia Package, the first time you attempt to run a script that uses it Julia will \"pre-compile\" that package to make it faster to launch scripts that use it in the future.  Unfortunately, when Julia does this it tries to write files within a directory in your home directory.  If multiple Julia instances are launched simultaneously they will ALL attempt to precompile the package and all attempt to overwrite eachother's pre-compiled files.  This leads to crazy errors.  To combat this: always make sure to run your MOOSE.jl scripts  without   mpiexec  first... to get MOOSE.jl (and everything it depends on) to pre-compile in serial.  Then you can run in parallel.  In fact: it's not necessary to run a full solve.  Simply create a file that has this in it (I call mine  compile.jl ):  using MOOSE  And then you can execute your real script like this:  julia compile.jl   mpiexec -n 4 julia myscript.jl  That will run the short  compile.jl  script in serial first... ensuring that MOOSE.jl is compiled and then launch the real script in parallel...", 
            "title": "Small Issue: \"Compiling\""
        }, 
        {
            "location": "/examples/parallel/#scalability", 
            "text": "Scalability of MOOSE.jl depends quite a lot on the linear solver/preconditioner you choose to use with PETSc and the particular problem you are solving.  That said, the finite-element assembly part of MOOSE.jl does scale well.  The result below is for a coupled set of 25 PDEs being assembled on a 400x400 element mesh on up to 3,072 processors.", 
            "title": "Scalability"
        }, 
        {
            "location": "/systems/kernel/", 
            "text": "Kernel\n\n\nNote: I also recommend reading \nFinite Elements The MOOSE Way\n over on the MOOSE Wiki for more information.\n\n\nA \nKernel\n represents a piece of physics.  Typically a Kernel will embody one or more terms in a partial differential equation (PDE).  It's useful to think of a Kernel as an \"operator\" that is then applied to variables within a PDE.  By utilizing multiple Kernels all operating on the same variables large, complex PDEs can be easily solved for.\n\n\nA Kernel has three major pieces:\n\n\n\n\nA \ntype\n that is a sub-type of \nKernel\n\n\nA \ncomputeQpResidual()\n function that works with that \ntype\n and computes the value of the Kernel at one quadrature point.\n\n\nAn \noptional\n \ncomputeQpJacobian()\n function computing the derivative of the residual at one quadrature point.\n\n\n\n\nThe \ncomputeQpJacobian()\n function is only optional if you're using Automatic Differentiation.  For more information see the \nAutomaticDifferentiation Example\n.\n\n\nKernel Sub-Type\n\n\nDefining a new Kernel starts by sub-typing \nKernel\n.  The sub-type \nmust\n include a member variable named \nu\n that is a \nVariable\n.  \nu\n is always the name for the Variable \nthis\n Kernel is operating on.  Here is an example from \nDiffusion.jl\n:\n\n\ntype Diffusion \n: Kernel\n    u::Variable\nend\n\n\n\n\nIn addition to defining \nu\n a Kernel can also take in other parameters.  For example, \nConvection\n in \nConvection.jl\n takes in a constant \"velocity vector\":\n\n\ntype Convection \n: Kernel\n    u::Variable\n\n    velocity::Vec{2, Float64}\nend\n\n\n\n\nThat \nvelocity\n can then be used in \ncomputeQpResidual()\n/\ncomputeQpJacobian()\n.\n\n\nFinally, the Kernel sub-type should also have member variables for \ncoupled\n variables like so:\n\n\ntype CoupledConvection \n: Kernel\n    u::Variable\n    other_var::Variable\nend\n\n\n\n\nResidual\n\n\nA Kernel embodies the weak form of a term in a PDE.  \ncomputeQpResidual()\n is where the value of that term is computed.\n\n\nWeak Form\n\n\nTo form the \"weak form\" of a PDE several steps must be taken:\n\n\n\n\nWrite down strong form of PDE.\n\n\nRearrange terms so that zero is on the right of the equals sign.\n\n\nMultiply the whole equation by a \"test\" function \\(\\phi\\).\n\n\nIntegrate the whole equation over the domain \\(\\Omega\\).\n\n\nIntegrate by parts (use the divergence theorem) to get the desired derivative order on your functions and simultaneously generate boundary integrals.\n\n\n\n\nLet's try it on an example: a \"Convection-Diffusion\" PDE:\n\n\n\n\n\n\nWrite the strong form of the equation: \\(- \\nabla\\cdot k\\nabla u + \\vec{\\beta} \\cdot \\nabla u = f  \\phantom{\\displaystyle \\int}\\)\n\n\n\n\n\n\nRearrange to get zero on the right-hand side: \\(- \\nabla\\cdot k\\nabla u + \\vec{\\beta} \\cdot \\nabla u - f = 0 \\phantom{\\displaystyle \\int}\\)\n\n\n\n\n\n\nMultiply by the test function \\(\\phi\\): \\(- \\phi \\left(\\nabla\\cdot k\\nabla u\\right) + \\phi\\left(\\vec{\\beta} \\cdot \\nabla u\\right) - \\phi f = 0 \\phantom{\\displaystyle \\int}\\)\n\n\n\n\n\n\nIntegrate over the domain \\(\\Omega\\): \\({-\\int_\\Omega\\phi \\left(\\nabla\\cdot k\\nabla u\\right)} + \\int_\\Omega\\phi\\left(\\vec{\\beta} \\cdot \\nabla u\\right) - \\int_\\Omega\\phi f = 0\\)\n\n\n\n\n\n\nApply the divergence theorem to the diffusion term: \\(\\int_\\Omega\\nabla\\phi\\cdot k\\nabla u - \\int_{\\partial\\Omega} \\phi \\left(k\\nabla u \\cdot \\hat{n}\\right) + \\int_\\Omega\\phi\\left(\\vec{\\beta} \\cdot \\nabla u\\right) - \\int_\\Omega\\phi f = 0\\)\n\n\n\n\n\n\nWrite in inner product notation. Each term of the equation will inherit from an existing MOOSE type as shown below.\n\n\n\n\n\n\n\\[\\underbrace{\\left(\\nabla\\phi, k\\nabla u \\right)}_{Kernel} -\n  \\underbrace{\\langle\\phi, k\\nabla u\\cdot \\hat{n} \\rangle}_{BoundaryCondition} +\n  \\underbrace{\\left(\\phi, \\vec{\\beta} \\cdot \\nabla u\\right)}_{Kernel} -\n  \\underbrace{\\left(\\phi, f\\right)}_{Kernel} = 0 \\phantom{\\displaystyle \\int}\\]\n\n\nFor this equation we would create/use three \nKernel\n objects and one \nBoundaryCondition\n.  The \"inner-product\" notation above shows what the \"residual\" should be for each term.  The \ncomputeQpResidual()\n function needs to compute what's inside each one of these integrals.\n\n\ncomputeQpResidual()\n\n\nCreating a \ncomputeQpResidual()\n function for a Kernel is done by specializing \ncomputeQpResidual()\n for the new Kernel:\n\n\n@inline function computeQpResidual(kernel::NewKernelType, qp::Int64, i::Int64)\n    return residual_computation\nend\n\n\n\n\nwhere \nNewKernelType\n represents the new type of Kernel you just create by sub-typing Kernel.  This utilizes Julia's \"multiple dispatch\" capability so that this new function will get called whenever a residual is needed for the new Kernel.\n\n\nqp\n is an index to use as the current quadrature point (for numerical integration) while \ni\n is the index of the current shape function.\n\n\nLaplacian Example\n\n\nLet's take a concrete example of a Laplacian operator.\n\n\n\n\nStrong form: \\(-\\nabla\\cdot\\nabla u\\)\n\n\nWeak form: \\(\\int_\\Omega \\nabla u \\cdot \\nabla \\phi\\)\n\n\n\n\nThen what goes in \ncomputeQpResidual()\n is: \\(\\nabla u \\cdot \\nabla \\phi\\) like so:\n\n\n@inline function computeQpResidual(kernel::Diffusion, qp::Integer, i::Integer)\n    u = kernel.u\n\n    return u.grad[qp] \u22c5 u.grad_phi[qp][i]\nend\n\n\n\n\nGetting \nu\n like that is not strictly necessary.  It's simply done to make the code a littler nicer (so that we're not repeating \nkernel.u\n all the time).\n\n\nJacobian\n\n\nA Jacobian is the derivative of the residual.  To define this for a Kernel we'll create a \ncomputeQpJacobian()\n function that computes the derivative of the residual with respect to one particular degree of freedom at one quadrature point.\n\n\nNote: Jacobians are NOT required if you're using Automatic Differentiation!  In that case these functions won't even be called!\n\n\nMath\n\n\nSince \\(u \\approx \\sum u_j \\phi_j\\) that implies that \\(\\frac{\\partial u}{\\partial u_k} = \\phi_k\\).  That is: the derivative of the variable with respect to one of its coefficients simply \"picks off\" the shape function that multiplies that coefficient.  The same applies to the gradient as well.\n\n\ncomputeQpJacobian()\n\n\nThe actual implementation is similar to \ncomputeQpResidual()\n:\n\n\n@inline function computeQpJacobian(kernel::NewKernelType, v::Variable, qp::Integer, i::Integer, j::Integer)::Float64\n    return jacobian_calculation\nend\n\n\n\n\nWhere \nv::Variable\n is the variable MOOSE wants the derivative with respect to and \nj::Integer\n is an index for the \"jth\" shape function (the one corresponding to the \"trial\" function: the one supporting the variable \nthis\n Kernel is acting on).\n\n\nWhat needs to be done is to use the \nid\n field in \nv\n to see which variable this variable is... and then if it's a variable that is used in this Kernel's residual computation then return the value of the derivative of the residual with respect to that variable.\n\n\nLet's do an example:\n\n\nExample\n\n\nContinuing with the Laplacian example from above...\n\n\n\n\nStrong form: \\(-\\nabla\\cdot\\nabla u\\)\n\n\nWeak form: \\(\\int_\\Omega \\nabla u \\cdot \\nabla \\phi\\)\n\n\nJacobian: \\(\\frac{\\partial}{\\partial u_j}\\int_\\Omega \\nabla u \\cdot \\nabla \\phi = \\int_\\Omega \\nabla \\phi \\cdot \\nabla \\phi\\)\n\n\n\n\nTo code that up looks like:\n\n\n@inline function computeQpJacobian(kernel::Diffusion, v::Variable, qp::Integer, i::Integer, j::Integer)::Float64\n    u = kernel.u\n\n    if u.id == v.id\n        return v.grad_phi[qp][j] \u22c5 u.grad_phi[qp][i]\n    end\n\n    return 0\nend\n\n\n\n\nYou can clearly see the \\(\\nabla \\phi \\cdot \\nabla \\phi\\) part... and it is only when MOOSE is looking for the derivative of this new Kernel with respect to the variable it's acting on (found by checking \nu.id == v.id\n).\n\n\nThe \nreturn 0\n at the end means that if MOOSE is looking for the derivative of this Kernel with respect to any other variable... then the value will always be zero (because there are no coefficients of that variable involved in the residual of this Kernel).", 
            "title": "Kernel"
        }, 
        {
            "location": "/systems/kernel/#kernel", 
            "text": "Note: I also recommend reading  Finite Elements The MOOSE Way  over on the MOOSE Wiki for more information.  A  Kernel  represents a piece of physics.  Typically a Kernel will embody one or more terms in a partial differential equation (PDE).  It's useful to think of a Kernel as an \"operator\" that is then applied to variables within a PDE.  By utilizing multiple Kernels all operating on the same variables large, complex PDEs can be easily solved for.  A Kernel has three major pieces:   A  type  that is a sub-type of  Kernel  A  computeQpResidual()  function that works with that  type  and computes the value of the Kernel at one quadrature point.  An  optional   computeQpJacobian()  function computing the derivative of the residual at one quadrature point.   The  computeQpJacobian()  function is only optional if you're using Automatic Differentiation.  For more information see the  AutomaticDifferentiation Example .", 
            "title": "Kernel"
        }, 
        {
            "location": "/systems/kernel/#kernel-sub-type", 
            "text": "Defining a new Kernel starts by sub-typing  Kernel .  The sub-type  must  include a member variable named  u  that is a  Variable .   u  is always the name for the Variable  this  Kernel is operating on.  Here is an example from  Diffusion.jl :  type Diffusion  : Kernel\n    u::Variable\nend  In addition to defining  u  a Kernel can also take in other parameters.  For example,  Convection  in  Convection.jl  takes in a constant \"velocity vector\":  type Convection  : Kernel\n    u::Variable\n\n    velocity::Vec{2, Float64}\nend  That  velocity  can then be used in  computeQpResidual() / computeQpJacobian() .  Finally, the Kernel sub-type should also have member variables for  coupled  variables like so:  type CoupledConvection  : Kernel\n    u::Variable\n    other_var::Variable\nend", 
            "title": "Kernel Sub-Type"
        }, 
        {
            "location": "/systems/kernel/#residual", 
            "text": "A Kernel embodies the weak form of a term in a PDE.   computeQpResidual()  is where the value of that term is computed.", 
            "title": "Residual"
        }, 
        {
            "location": "/systems/kernel/#weak-form", 
            "text": "To form the \"weak form\" of a PDE several steps must be taken:   Write down strong form of PDE.  Rearrange terms so that zero is on the right of the equals sign.  Multiply the whole equation by a \"test\" function \\(\\phi\\).  Integrate the whole equation over the domain \\(\\Omega\\).  Integrate by parts (use the divergence theorem) to get the desired derivative order on your functions and simultaneously generate boundary integrals.   Let's try it on an example: a \"Convection-Diffusion\" PDE:    Write the strong form of the equation: \\(- \\nabla\\cdot k\\nabla u + \\vec{\\beta} \\cdot \\nabla u = f  \\phantom{\\displaystyle \\int}\\)    Rearrange to get zero on the right-hand side: \\(- \\nabla\\cdot k\\nabla u + \\vec{\\beta} \\cdot \\nabla u - f = 0 \\phantom{\\displaystyle \\int}\\)    Multiply by the test function \\(\\phi\\): \\(- \\phi \\left(\\nabla\\cdot k\\nabla u\\right) + \\phi\\left(\\vec{\\beta} \\cdot \\nabla u\\right) - \\phi f = 0 \\phantom{\\displaystyle \\int}\\)    Integrate over the domain \\(\\Omega\\): \\({-\\int_\\Omega\\phi \\left(\\nabla\\cdot k\\nabla u\\right)} + \\int_\\Omega\\phi\\left(\\vec{\\beta} \\cdot \\nabla u\\right) - \\int_\\Omega\\phi f = 0\\)    Apply the divergence theorem to the diffusion term: \\(\\int_\\Omega\\nabla\\phi\\cdot k\\nabla u - \\int_{\\partial\\Omega} \\phi \\left(k\\nabla u \\cdot \\hat{n}\\right) + \\int_\\Omega\\phi\\left(\\vec{\\beta} \\cdot \\nabla u\\right) - \\int_\\Omega\\phi f = 0\\)    Write in inner product notation. Each term of the equation will inherit from an existing MOOSE type as shown below.    \\[\\underbrace{\\left(\\nabla\\phi, k\\nabla u \\right)}_{Kernel} -\n  \\underbrace{\\langle\\phi, k\\nabla u\\cdot \\hat{n} \\rangle}_{BoundaryCondition} +\n  \\underbrace{\\left(\\phi, \\vec{\\beta} \\cdot \\nabla u\\right)}_{Kernel} -\n  \\underbrace{\\left(\\phi, f\\right)}_{Kernel} = 0 \\phantom{\\displaystyle \\int}\\]  For this equation we would create/use three  Kernel  objects and one  BoundaryCondition .  The \"inner-product\" notation above shows what the \"residual\" should be for each term.  The  computeQpResidual()  function needs to compute what's inside each one of these integrals.", 
            "title": "Weak Form"
        }, 
        {
            "location": "/systems/kernel/#computeqpresidual", 
            "text": "Creating a  computeQpResidual()  function for a Kernel is done by specializing  computeQpResidual()  for the new Kernel:  @inline function computeQpResidual(kernel::NewKernelType, qp::Int64, i::Int64)\n    return residual_computation\nend  where  NewKernelType  represents the new type of Kernel you just create by sub-typing Kernel.  This utilizes Julia's \"multiple dispatch\" capability so that this new function will get called whenever a residual is needed for the new Kernel.  qp  is an index to use as the current quadrature point (for numerical integration) while  i  is the index of the current shape function.", 
            "title": "computeQpResidual()"
        }, 
        {
            "location": "/systems/kernel/#laplacian-example", 
            "text": "Let's take a concrete example of a Laplacian operator.   Strong form: \\(-\\nabla\\cdot\\nabla u\\)  Weak form: \\(\\int_\\Omega \\nabla u \\cdot \\nabla \\phi\\)   Then what goes in  computeQpResidual()  is: \\(\\nabla u \\cdot \\nabla \\phi\\) like so:  @inline function computeQpResidual(kernel::Diffusion, qp::Integer, i::Integer)\n    u = kernel.u\n\n    return u.grad[qp] \u22c5 u.grad_phi[qp][i]\nend  Getting  u  like that is not strictly necessary.  It's simply done to make the code a littler nicer (so that we're not repeating  kernel.u  all the time).", 
            "title": "Laplacian Example"
        }, 
        {
            "location": "/systems/kernel/#jacobian", 
            "text": "A Jacobian is the derivative of the residual.  To define this for a Kernel we'll create a  computeQpJacobian()  function that computes the derivative of the residual with respect to one particular degree of freedom at one quadrature point.  Note: Jacobians are NOT required if you're using Automatic Differentiation!  In that case these functions won't even be called!", 
            "title": "Jacobian"
        }, 
        {
            "location": "/systems/kernel/#math", 
            "text": "Since \\(u \\approx \\sum u_j \\phi_j\\) that implies that \\(\\frac{\\partial u}{\\partial u_k} = \\phi_k\\).  That is: the derivative of the variable with respect to one of its coefficients simply \"picks off\" the shape function that multiplies that coefficient.  The same applies to the gradient as well.", 
            "title": "Math"
        }, 
        {
            "location": "/systems/kernel/#computeqpjacobian", 
            "text": "The actual implementation is similar to  computeQpResidual() :  @inline function computeQpJacobian(kernel::NewKernelType, v::Variable, qp::Integer, i::Integer, j::Integer)::Float64\n    return jacobian_calculation\nend  Where  v::Variable  is the variable MOOSE wants the derivative with respect to and  j::Integer  is an index for the \"jth\" shape function (the one corresponding to the \"trial\" function: the one supporting the variable  this  Kernel is acting on).  What needs to be done is to use the  id  field in  v  to see which variable this variable is... and then if it's a variable that is used in this Kernel's residual computation then return the value of the derivative of the residual with respect to that variable.  Let's do an example:", 
            "title": "computeQpJacobian()"
        }, 
        {
            "location": "/systems/kernel/#example", 
            "text": "Continuing with the Laplacian example from above...   Strong form: \\(-\\nabla\\cdot\\nabla u\\)  Weak form: \\(\\int_\\Omega \\nabla u \\cdot \\nabla \\phi\\)  Jacobian: \\(\\frac{\\partial}{\\partial u_j}\\int_\\Omega \\nabla u \\cdot \\nabla \\phi = \\int_\\Omega \\nabla \\phi \\cdot \\nabla \\phi\\)   To code that up looks like:  @inline function computeQpJacobian(kernel::Diffusion, v::Variable, qp::Integer, i::Integer, j::Integer)::Float64\n    u = kernel.u\n\n    if u.id == v.id\n        return v.grad_phi[qp][j] \u22c5 u.grad_phi[qp][i]\n    end\n\n    return 0\nend  You can clearly see the \\(\\nabla \\phi \\cdot \\nabla \\phi\\) part... and it is only when MOOSE is looking for the derivative of this new Kernel with respect to the variable it's acting on (found by checking  u.id == v.id ).  The  return 0  at the end means that if MOOSE is looking for the derivative of this Kernel with respect to any other variable... then the value will always be zero (because there are no coefficients of that variable involved in the residual of this Kernel).", 
            "title": "Example"
        }, 
        {
            "location": "/systems/boundary_condition/", 
            "text": "BoundaryCondition\n\n\nA BoundaryCondition is similar to a \nKernel\n.  It is formed out of the weak form of a PDE (see the \nexplanation in Kernel\n) and computes a residual and a Jacobian.\n\n\nHowever, there are two types of BoundaryCondition objects: \nIntegratedBC\n and \nNodalBC\n.  In general, an \nIntegratedBC\n should be used to represent boundary integrals within a weak form.  The \nNodalBC\n is mainly used for \"Dirichlet\" or \"Type-1\" or \"Essential\" boundary conditions.\n\n\nNodalBC\n\n\nAs mentioned above a \nNodalBC\n represents a \"Dirichlet\" boundary condition.  That is, a condition of the type \\(u = value\\) on the boundary nodes.  In MOOSE that value can either be a constant, or even a nonlinear function of the solution variables at that node.  One restriction on \nNodalBC\n objects is that they do \nnot\n have access to gradient information (because gradients are typically discontinuous at the nodes).\n\n\nCreating a NodalBC\n\n\nTo create a new NodalBC you need to create a sub-type of \nNodalBC\n like so:\n\n\n Boundary condition expressing `u = value` at a set of nodes \n\ntype DirichletBC \n: NodalBC\n    \n The variable the BC will be applied to \n\n    u::Variable\n\n    \n The boundary ID to apply this BC to \n\n    bids::Array{Int64}\n\n    \n The value on the boundary \n\n    value::Float64\nend\n\n\n\n\nHere we are showing the actual code from \nDirichletBC.jl\n for implementing a \\(u=v\\) type BC.  A BoundaryCondition \nmust\n provide \nu\n: the Variable the BC is being applied to and \nbids::Array{Int64}\n: the boundary IDs in the mesh where the BC will be applied.\n\n\nAfter the type has been created then a \ncomputeQpResidual()\n function needs to be created similar to \nKernels\n.  However, to form the residual for a NodalBC you simply take the \\(u=v\\) equation and move everything to the left hand side: \\(u-v=0\\).  What is on the left hand side is the \"residual\" and can be coded up like so:\n\n\nfunction computeResidual(bc::DirichletBC)\n    u = bc.u\n\n    return u.nodal_value - bc.value\nend\n\n\n\n\nSimilar to Kernels a \ncomputeQpJacobian()\n statement can be provided.  It is only optional if you are using \nAutomatic Differentiation\n.  Since the value of any Lagrange shape function at a node is always 1... \\(\\frac{\\partial u}{\\partial u_j} = 1\\) at the node.  Therefore:\n\n\nfunction computeJacobian(bc::DirichletBC, v::Variable{Float64})\n    u = bc.u\n\n    if u.id == v.id\n        return 1.\n    end\n\n    return 0.\nend\n\n\n\n\nJust as in \nKernels\n you should always \nreturn 0\n in the case where MOOSE is looking for a derivative with respect to a variable this BoundaryCondition is not operating on.", 
            "title": "BoundaryCondition"
        }, 
        {
            "location": "/systems/boundary_condition/#boundarycondition", 
            "text": "A BoundaryCondition is similar to a  Kernel .  It is formed out of the weak form of a PDE (see the  explanation in Kernel ) and computes a residual and a Jacobian.  However, there are two types of BoundaryCondition objects:  IntegratedBC  and  NodalBC .  In general, an  IntegratedBC  should be used to represent boundary integrals within a weak form.  The  NodalBC  is mainly used for \"Dirichlet\" or \"Type-1\" or \"Essential\" boundary conditions.", 
            "title": "BoundaryCondition"
        }, 
        {
            "location": "/systems/boundary_condition/#nodalbc", 
            "text": "As mentioned above a  NodalBC  represents a \"Dirichlet\" boundary condition.  That is, a condition of the type \\(u = value\\) on the boundary nodes.  In MOOSE that value can either be a constant, or even a nonlinear function of the solution variables at that node.  One restriction on  NodalBC  objects is that they do  not  have access to gradient information (because gradients are typically discontinuous at the nodes).", 
            "title": "NodalBC"
        }, 
        {
            "location": "/systems/boundary_condition/#creating-a-nodalbc", 
            "text": "To create a new NodalBC you need to create a sub-type of  NodalBC  like so:   Boundary condition expressing `u = value` at a set of nodes  \ntype DirichletBC  : NodalBC\n      The variable the BC will be applied to  \n    u::Variable\n\n      The boundary ID to apply this BC to  \n    bids::Array{Int64}\n\n      The value on the boundary  \n    value::Float64\nend  Here we are showing the actual code from  DirichletBC.jl  for implementing a \\(u=v\\) type BC.  A BoundaryCondition  must  provide  u : the Variable the BC is being applied to and  bids::Array{Int64} : the boundary IDs in the mesh where the BC will be applied.  After the type has been created then a  computeQpResidual()  function needs to be created similar to  Kernels .  However, to form the residual for a NodalBC you simply take the \\(u=v\\) equation and move everything to the left hand side: \\(u-v=0\\).  What is on the left hand side is the \"residual\" and can be coded up like so:  function computeResidual(bc::DirichletBC)\n    u = bc.u\n\n    return u.nodal_value - bc.value\nend  Similar to Kernels a  computeQpJacobian()  statement can be provided.  It is only optional if you are using  Automatic Differentiation .  Since the value of any Lagrange shape function at a node is always 1... \\(\\frac{\\partial u}{\\partial u_j} = 1\\) at the node.  Therefore:  function computeJacobian(bc::DirichletBC, v::Variable{Float64})\n    u = bc.u\n\n    if u.id == v.id\n        return 1.\n    end\n\n    return 0.\nend  Just as in  Kernels  you should always  return 0  in the case where MOOSE is looking for a derivative with respect to a variable this BoundaryCondition is not operating on.", 
            "title": "Creating a NodalBC"
        }
    ]
}